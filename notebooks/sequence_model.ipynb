{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence models for text classification\n",
    "<hr>\n",
    "\n",
    "1. load, split, and preprocess data \n",
    "2. define network structure, set hyper-params, compile  \n",
    "3. train classifier \n",
    "4. generate predictions on holdout set \n",
    "5. evaluate performance \n",
    "6. Appendix: transfer learning with pre-trained word embeddings\n",
    "  - 6.1 load pre-trained embeddings\n",
    "  - 6.2 tokenize and matricize text data (replace tokens with their embeddings)\n",
    "  - 6.3 adapt network structure for input matrices \n",
    "  - 6.4 train classifier, generate predictions, evaluate performance \n",
    "\n",
    "\n",
    "**`TODO`**:\n",
    "- clean up all cells\n",
    "- add notes where nec\n",
    "- reintroduce plotting part at end\n",
    "\n",
    "**`TODO`** -- insert discussion about:\n",
    "- difference btwn sequence models + dtm-based models\n",
    "- feature \"learning\" versus engineering\n",
    "- notable hypers that dont appear in other situations\n",
    "- role of embeddings + decisions to make w.r.t. them in seq mods\n",
    "- decide whether to have separate rnn/gru/lstm nb's or just mention each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load, split, and preprocess data\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read in the movie reviews dataset `imdb_decoded.csv`, displaying the first few rows. Note that it has a field for train/test (`'subset'`), to make splitting the data easier. Note also that it has fields `'length'` and `'length_bin'`, which specify the length (in words) and length quartile the review belongs to (quartiles calculated across all 50k reviews). \n",
    "\n",
    "> Note: The data in `dat` is a re-formatted version of the IMDB movie reviews sentiment dataset, which can be accessed in integer-encoded format via `keras.datasets.load_imdb()`. If you don't have the file `imdb_decoded.csv`, just run the script `expt1_prep_imdb_data.py` to generate it (make sure `outfile` points to a valid filepath). \n",
    "\n",
    "<!-- \n",
    "  `dat` differs from the value of `keras.datasets.load_imdb()` only in format and encoding: the present version is a single rectangular data frame, the reviews have been re-encoded as English text, each review's length (in words) and train/test status is available, and the length quartile of each review is also available (in the `length_bin` field, which has values in `[0,1,2,3,4]`). \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>length</th>\n",
       "      <th>length_bin</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>217</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>this film was just brilliant casting location ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>big hair big boobs bad music and a giant safet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>this has to be one of the worst films of the 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>549</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>the scots excel at storytelling the traditiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>worst mistake of my life br br i picked this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subset  length  length_bin  label  \\\n",
       "0  train     217           2      1   \n",
       "1  train     188           2      0   \n",
       "2  train     140           1      0   \n",
       "3  train     549           3      1   \n",
       "4  train     146           1      0   \n",
       "\n",
       "                                                text  \n",
       "0  this film was just brilliant casting location ...  \n",
       "1  big hair big boobs bad music and a giant safet...  \n",
       "2  this has to be one of the worst films of the 1...  \n",
       "3  the scots excel at storytelling the traditiona...  \n",
       "4  worst mistake of my life br br i picked this m...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fname = '../experiments/expt1/data/imdb_decoded.csv'\n",
    "\n",
    "dat = pd.read_csv(fname)\n",
    "dat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 25k train reviews and 25k test reviews. Average review length is approximately the same across train and test subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 25000 (avg 238 words)\n",
      "test size: 25000 (avg 230 words)\n"
     ]
    }
   ],
   "source": [
    "train, test = dat[dat.subset=='train'], dat[dat.subset=='test']\n",
    "\n",
    "txt_train, y_train = list(train.text), list(train.label)\n",
    "txt_test, y_test = list(test.text), list(test.label)\n",
    "\n",
    "print(f'train size: {len(txt_train)} (avg {round(train.length.mean())} words)')\n",
    "print(f'test size: {len(txt_test)} (avg {round(test.length.mean())} words)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tokenize the texts, restricting vocabulary to the words appearing in the train subset. To reduce the risk of learning useless patterns from rare words, vocabulary is also restricted to the `max_features` most frequent words (within the train set). \n",
    "\n",
    "Because we will be learning feature representations instead of engineering them, each review must have a uniform length. To achieve this, reviews longer than `maxlen` tokens are truncated to the first `maxlen` tokens, and reviews shorter than `maxlen` tokens are left-padded with the reserved dummy index `0` until they are `maxlen`-many tokens long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# set text-related hyper params \n",
    "maxlen = 150\n",
    "max_features = 10000\n",
    "\n",
    "\n",
    "# instantiate Tokenizer class (`num_words` to restrict vocab size)\n",
    "# extract vocab and count words (makes several attrs available)\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(txt_train)\n",
    "\n",
    "# integer encode the docs\n",
    "x_train = tokenizer.texts_to_sequences(txt_train)\n",
    "x_test = tokenizer.texts_to_sequences(txt_test)\n",
    "\n",
    "# pad the sequences (default params `padding='pre', truncating='pre'`)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: It is sometimes also necessary to vectorize the labels in preparation for working with neural networks. But in the present case, we are doing binary classification so a single value (`0` or `1`) is fine. If we were predicting number of stars (from 1 to 5) instead of positive/negative, then we would one-hot encode each label so that e.g. `'2 stars'` would be encoded as `[0, 1, 0, 0, 0]`, `'4 stars'` would be encoded as `[0, 0, 0, 1, 0]`, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define network structure, set hyper-params, compile\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the network structure and *compile* the network. \n",
    "\n",
    "Note that the structure and definition of a network makes no reference to the data, other than that the input dimension of the input layer must be compatible with the shape of the data, and that the output layer must produce a value that makes sense in the context of the labels (in the network defined here, the output layer produces a single probability, which makes sense since we are doing binary classification). \n",
    "\n",
    "\n",
    "#### Network structure\n",
    "There are infinitely many structures we could choose for a neural net-based classifier. The best structure depends on a huge number of factors, some related to the data (e.g. binary/multi-class, text or numeric input, sequence or non-), some related to practical considerations (e.g. performance for real-time systems), and so on. \n",
    "\n",
    "Every neural network has an *input layer* and an *output layer*, but there can be any number of hidden layers between the input and output. Hidden layers can have varying numbers of nodes, the connections between the nodes can vary, and layers can be endowed with special properties like dropout. The interested reader is referred to [Chollet (2018)](TODO--link-to-DLwP) for more information. \n",
    "\n",
    "Here we will define a network with the following components: \n",
    "\n",
    "- **input layer** -- an \"embedding\" layer with input dimension `max_features` (one node for each vocabulary item) and output dimension `hidden_dim` (a tunable hyper-parameter)\n",
    "- **hidden layer 1** -- an LSTM layer with dropout and recurrent dropout (whose values can also be tuned); input and output dimensions are both `hidden_dim` \n",
    "- **output layer** -- a one-unit output layer with activation function `out_activation` (produces probabilities) \n",
    "\n",
    "#### Compiling a network\n",
    "Whereas the layers and nodes of a network are characterized mathematically, compiling a network is something that's only necessary because the network has been described using a computer. \n",
    "\n",
    "Compiling a network configures it for training, by supplying information about how performance should be measured (loss) and which algorithm should be used for optimizing the parameter weights (optimizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# training-related hypers (how much data and for how long)\n",
    "epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "# training-related hypers (objective and how to maximize it)\n",
    "loss = 'binary_crossentropy'\n",
    "train_metrics = ['accuracy']\n",
    "optimizer = 'adam'\n",
    "\n",
    "# hypers for network layout + structure  \n",
    "hidden_dim = 100\n",
    "rec_dropout = 0.2\n",
    "lstm_dropout = 0.2\n",
    "out_activation = 'sigmoid'\n",
    "\n",
    "\n",
    "# define the network structure \n",
    "model = Sequential()\n",
    "# TODO: EITHER TOSS THIS LAYER (+ ADJUST AS NEEDED) OR EXPLAIN IT!!! \n",
    "model.add(Embedding(max_features, hidden_dim))\n",
    "model.add(LSTM(hidden_dim, dropout=lstm_dropout,recurrent_dropout=rec_dropout))\n",
    "model.add(Dense(1, activation=out_activation))\n",
    "\n",
    "# compile the network \n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model objects of class `keras.models.Sequential` have a `.summary()` method, which you can call to see some basic info about the layers of your network, their shapes, and the number of trainable parameters they contribute to the overall model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train classifier \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will train the classifier in mini-batches of `batch_size`, for `epochs`-many runs through the entire training dataset. Higher values for `epoch` are more likely to over-fit to the training data. \n",
    "\n",
    "Note also that when we specify `validation_split`, a portion of the data is not used for training, but rather to quantify the model's out-of-sample prediction accuracy. At each epoch, you will see the model's prediction accuracy on a (`valset_prop`\\*100)% subset of the samples in `x_train` -- none of which is seen during actual training. \n",
    "\n",
    "Setting aside a portion of the training data for evaluation *during training* is important because it gives you an idea of how well your model will generalize to cases outside of the data it was trained on. This principle isn't specific to neural nets. \n",
    "\n",
    "Crucially, the data set aside for validation during training is *not* the same as the test set (here, `x_test` and `y_test`). The test set is meant to quantify performance on a tuned/optimized model, and should be used sparingly (ideally, only once \"the official model\" has been selected). Instead, assess the impact of parameter tweaks by looking at how the validation accuracy changes over epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "15000/15000 [==============================] - 68s 5ms/step - loss: 0.5905 - acc: 0.6932 - val_loss: 0.4635 - val_acc: 0.7953\n",
      "Epoch 2/5\n",
      "15000/15000 [==============================] - 56s 4ms/step - loss: 0.3699 - acc: 0.8481 - val_loss: 0.3791 - val_acc: 0.8332\n",
      "Epoch 3/5\n",
      "15000/15000 [==============================] - 56s 4ms/step - loss: 0.2735 - acc: 0.8935 - val_loss: 0.3548 - val_acc: 0.8569\n",
      "Epoch 4/5\n",
      "15000/15000 [==============================] - 57s 4ms/step - loss: 0.2220 - acc: 0.9165 - val_loss: 0.3865 - val_acc: 0.8410\n",
      "Epoch 5/5\n",
      "15000/15000 [==============================] - 61s 4ms/step - loss: 0.2016 - acc: 0.9278 - val_loss: 0.4397 - val_acc: 0.8385\n"
     ]
    }
   ],
   "source": [
    "# prop to set aside for accuracy calc after each epoch \n",
    "valset_prop = .40\n",
    "\n",
    "# train the model (returns a keras.callbacks.History object)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, validation_split=valset_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the accuracy and loss curves, to see how validation performance compares to training performance across training epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# consider using magic cmd `%matplotlib notebook` instead \n",
    "\n",
    "from plotting_util import plot_keras_loss_accuracy_curves\n",
    "\n",
    "### TODO: spruce up the plots (cant even see w dark jlab theme...)\n",
    "plot_keras_loss_accuracy_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate predictions on holdout set \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate model predictions on the test set using the model's `.predict()` method. \n",
    "\n",
    "Even though the model was trained for binary classification, `model.predict()` will return a probability in [0, 1] when given a piece of text as input. This should be interpreted as the probability that the sample belongs to the positive class (\"positive\" in the sense of \"corresponding to the value `1`\"). \n",
    "\n",
    "Here, we simply flatten the probabilities so that values .5 and above are assigned `True` (positive sentiment), and values below .5 are assigned `False` (negative sentiment). \n",
    "\n",
    "> Note: We could just as easily have used the method `model.predict_classes()`, which would give us the flattened (binary) predictions directly. But it can be informative to have the intermediate, probabilistic predictions if you want to introspect about why the model makes the predictions it does or how \"confident\" it is in a specific prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict() method returns probabilities \n",
    "# (use `model.predict_classes(x_test)` to get pre-flattened categorical preds)\n",
    "preds_test = model.predict(x_test)\n",
    "preds_test_bool = [bool(pred >= .5) for pred in preds_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate model performance \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of a machine learning model can be evaluated in many ways. Model evaluation is no different for neural networks than it is for any other class of model: we compare our model predictions on holdout data to the actual labels/ground truth on those same data points, summarizing the comparison with some metric(s) of our choosing. \n",
    "\n",
    "Keras model objects also have a `model.evaluate()` method, which will calculate a score by aggregating the value of some performance metric over mini-batches (see [this SO post for a quick summary](https://stackoverflow.com/a/44488571/6678726)). \n",
    "\n",
    "Here, we'll demonstrate use of the `model.evaluate()` method, and then move on to the more familiar and simple metrics of precision, recall, and F1-score. We'll also print a [confusion table](https://en.wikipedia.org/wiki/Confusion_matrix) to see how many data points in the test set fall into the categories of true positive, true negative, false positive, and false negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 21s 838us/step\n",
      "test loss: 0.4483\n",
      "test accuracy: 0.8338\n"
     ]
    }
   ],
   "source": [
    "### TODO: clean up -- first value is just the loss! can use instead smthg like e.g. \n",
    "### dict(zip(model.metrics_names, model.evaluate(x=test_seqs, y=test.label)))\n",
    "#####################################################################################\n",
    "\n",
    "# note that evaluation also happens in batches, so can take a min to run \n",
    "loss_, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print(f'test loss: {round(loss_, 4)}')\n",
    "print(f'test accuracy: {round(accuracy, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 0.8742\n",
      "recall_score: 0.7798\n",
      "f1_score: 0.8243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# evaluate model performance on full test set with familiar metrics\n",
    "metrics = [precision_score, recall_score, f1_score]\n",
    "\n",
    "for metric in metrics:\n",
    "  print(f'{metric.__name__}: {round(metric(y_test, preds_test_bool), 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set confusion matrix: \n",
      "[[11097  1403]\n",
      " [ 2753  9747]]\n",
      "count (% of total) for each label-pred combo:\n",
      "  >> true neg:  11097 (44.4%)\n",
      "  >> false pos: 1403 (5.6%)\n",
      "  >> false neg: 2753 (11.0%)\n",
      "  >> true pos:  9747 (39.0%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from plotting_util import human_readable_confusion_table\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, preds_test_bool)\n",
    "print('test set confusion matrix: ', conf_mat, sep='\\n')\n",
    "\n",
    "human_readable_confusion_table(y_test, preds_test_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Appendix: Transfer learning with pre-trained word embeddings\n",
    "<hr>\n",
    "\n",
    "**`TODO`**: \n",
    "- finish this sec!\n",
    "- clean up chunks in this sec!\n",
    "- write narrative for this sec!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Load pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 43981 300-d embeddings\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# just using these embs for convenience \n",
    "embs_file = '../experiments/expt1/data/pruned.word2vec.txt'\n",
    "embs = KeyedVectors.load_word2vec_format(embs_file, binary=False)\n",
    "\n",
    "vec_size = embs.vector_size\n",
    "\n",
    "print(f'loaded {len(embs.vocab)} {vec_size}-d embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info about the embedding for 'dog':\n",
      "  >> type: <class 'numpy.ndarray'>\n",
      "  >> len: 300\n",
      "  >> head: [ 0.0171981  -0.00749344 -0.057982    0.054051   -0.0283358 ]\n"
     ]
    }
   ],
   "source": [
    "# example of a word2vec word embedding\n",
    "dog_vector = embs['dog']\n",
    "\n",
    "funcs = dict(type=type, len=len, head=lambda x: x[:5])\n",
    "\n",
    "print('info about the embedding for \\'dog\\':')\n",
    "for name, func in funcs.items():\n",
    "  print(f'  >> {name}: {func(dog_vector)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab items: 88581\n",
      "vocab items for which we have a vector: 24094\n"
     ]
    }
   ],
   "source": [
    "# get a dict of embeddings, restricted to train vocab \n",
    "vocab_embs = {word: embs[word] \n",
    "              for word in embs.vocab.keys()\n",
    "              if  word in tokenizer.word_counts.keys()}\n",
    "\n",
    "print(f'total vocab items: {len(tokenizer.word_counts.keys())}')\n",
    "print(f'vocab items for which we have a vector: {len(vocab_embs.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Prepare embedding matrix/matrices\n",
    "\n",
    "> for ref see the relevant DLwP sec + also [this keras walkthru](https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py) \n",
    "\n",
    "> see also [this fasttext example](https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py) w uni/bigram embs (v nice ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "available_words = list(vocab_embs.keys())\n",
    "embs_matrix = np.zeros(shape=(max_features, vec_size))\n",
    "\n",
    "list(tokenizer.word_index.items())[:3]\n",
    "\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if idx < max_features and word in available_words:\n",
    "        embs_matrix[idx] = vocab_embs[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of embedding matrix: (10000, 300)\n",
      "note that 0th element is empty (bc reserved index)\n",
      "\n",
      "1th word idx (and embedding vector) is 'the'\n",
      "first few elems of 1th embedding vector:\n",
      "  >> [ 0.0746853  0.0979106  0.0464506  0.0498661 -0.062845 ]\n"
     ]
    }
   ],
   "source": [
    "print(f'dimensions of embedding matrix: {embs_matrix.shape}')\n",
    "print(f'note that 0th element is empty (bc reserved index)\\n')\n",
    "print(f'1th word idx (and embedding vector) is \\'{tokenizer.index_word.get(1)}\\'')\n",
    "print(f'first few elems of 1th embedding vector:\\n  >> {embs_matrix[1,:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Adapt network structure \n",
    "> (for matrix inputs instead of vector inputs)\n",
    "\n",
    "1. figure out what input layer needs to be like\n",
    "2. make it like that (mite need to \"freeze\" weights at some pt)\n",
    "3. compile model + print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,160,501\n",
      "Trainable params: 160,501\n",
      "Non-trainable params: 3,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features, vec_size, input_length=maxlen))\n",
    "# model2.add(Flatten()) # if LSTM is next layer, no flatten (??)\n",
    "model2.add(LSTM(hidden_dim, dropout=lstm_dropout, recurrent_dropout=rec_dropout))\n",
    "model2.add(Dense(1, activation=out_activation))\n",
    "\n",
    "# need to freeze the embedding weights \n",
    "model2.layers[0].set_weights([embs_matrix]) \n",
    "model2.layers[0].trainable = False\n",
    "\n",
    "model2.compile(loss=loss, optimizer=optimizer, metrics=train_metrics)\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Train/predict/evaluate \n",
    "> (train classifier, generate predictions, evaluate performance)\n",
    "\n",
    "1. train model\n",
    "2. plot curves\n",
    "3. generate preds\n",
    "4. calculate .evaluate() metrics + std metrics + conf mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "15000/15000 [==============================] - 100s 7ms/step - loss: 0.6310 - acc: 0.6582 - val_loss: 0.5610 - val_acc: 0.7395\n",
      "Epoch 2/2\n",
      "15000/15000 [==============================] - 101s 7ms/step - loss: 0.5465 - acc: 0.7393 - val_loss: 0.5157 - val_acc: 0.7637\n"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "\n",
    "# TODO: setting epochs to 2 or 3 for quick dev!\n",
    "#       when finalizing, just use `epochs` \n",
    "history2 = model2.fit(x_train, y_train,\n",
    "                      batch_size=batch_size, epochs=2, \n",
    "                      validation_split=valset_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate preds on test set \n",
    "preds_test2 = model2.predict(x_test)\n",
    "preds_test2_bool = [bool(pred >= .5) for pred in preds_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 0.8258\n",
      "recall_score: 0.6722\n",
      "f1_score: 0.7411\n",
      "test set confusion matrix: \n",
      "[[10728  1772]\n",
      " [ 4098  8402]]\n",
      "count (% of total) for each label-pred combo:\n",
      "  >> true neg:  10728 (42.9%)\n",
      "  >> false pos: 1772 (7.1%)\n",
      "  >> false neg: 4098 (16.4%)\n",
      "  >> true pos:  8402 (33.6%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance \n",
    "for metric in metrics:\n",
    "  print(f'{metric.__name__}: {round(metric(y_test, preds_test2_bool), 4)}')\n",
    "\n",
    "conf_mat2 = confusion_matrix(y_test, preds_test2_bool)\n",
    "print('test set confusion matrix: ', conf_mat2, sep='\\n')\n",
    "\n",
    "human_readable_confusion_table(y_test, preds_test2_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><hr>**[DONEZO HERE, JUST CLEAN UP BELOW + MAYBE ADD A LIL PROSE]**\n",
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "### SCRAPS AND ACTIVE `TODO` ITEMS \n",
    "\n",
    "> THIS SHOULD ALL BE DESTROYED/MOVED BEFORE FINAL!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TODO: FIND PAD CHAR FOR NARRATIVE TXT IN SEC 1!!! \n",
    "# (look for a short review + find its prepped version)\n",
    "# [r for r in x_train if len(r.split()) < 150][:5]\n",
    "# dat.length\n",
    "# dat.text[11]\n",
    "# [r for r in x_train if len(r.split()) < 150][:5]\n",
    "# dat.length\n",
    "# dat.text[11]\n",
    "# dat\n",
    "# \n",
    "\n",
    "### TODO: EFFICIENT SHOW REVIEW(S) BASED ON SUBSTRING \n",
    "# ss = 'when i rented this'\n",
    "# [r for r in x_test if ss in x_test]\n",
    "# x_train[0]\n",
    "# \n",
    "\n",
    "### TODO: QUICK WAY TO LOOK AT THE TEXT OF A BAD PRED \n",
    "# # currently cant do this bc just have one var from `dat` to seq's \n",
    "# [*zip(preds_test_bool[:10], y_test[:10])]\n",
    "# x_test[3]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OLD, NOT USED]\n",
    "<hr>\n",
    "\n",
    "#### 6.2.2 Token seq ==> embedding matrix (slow + prob not worth it...)\n",
    "\n",
    "<br>\n",
    "encode each doc as a matrix of embeddings:\n",
    "1. generate (maxlen, vec_len) matrix for each text \n",
    "2. that's the input data -- 3d array w shape: (len(x_train), maxlen, vec_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: BELOW IS JUST WHAT'S HAPPENING BEHIND THE SCENES W KERAS EMB LAYER\n",
    "\n",
    "## Define funcs to construct 3-d embeddings array from tokenized docs \n",
    "## TODO: DETERMINE IF THIS WILL MAKE IT INTO FINAL, THEN DEAL WITH IT \n",
    "## WARNING: VERY SLOW IMPLEMENTATION! LOOK ELSEWHERE \n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def token_idxs_to_embedding_matrix(tokens, index_word, embs, vec_size):\n",
    "    '''transform a sequence of token indices into an embedding matrix \n",
    "    NOTE: `embs` must be dict, not instance of gensim.models.KeyedVectors!!\n",
    "    \n",
    "    tokens_ = [1,2,3]\n",
    "    index_word_ = {1: 'the', 2: 'dog', 3: 'barked'}\n",
    "    embs_ = {'the': [.1,.2,.3], 'dog': [.4,.5,.6], 'barked': [.7,.8,.9]}\n",
    "    token_idxs_to_embedding_matrix(tokens_, index_word_, embs_, 3)\n",
    "    '''\n",
    "    out = np.zeros(shape=(len(tokens), vec_size)) # or use `maxlen`\n",
    "    \n",
    "    available_vecs = list(embs.keys())\n",
    "    \n",
    "    for idx in range(len(tokens)):\n",
    "        word = index_word.get(tokens[idx])\n",
    "        if word in available_vecs:\n",
    "            out[idx, ] = embs[word]\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def make_embeddings_array3d(token_seqs, index_word, embs, maxlen, vec_size):\n",
    "    '''transform a seq of token seqs into a seq of embedding matrices \n",
    "    \n",
    "    token_seqs = [[1,2,3,1], [3,1,2,1], [2,1,3,1]]\n",
    "    index_word_ = {1: 'the', 2: 'dog', 3: 'barked'}\n",
    "    embs_ = {'the': [.1,.2,.3], 'dog': [.4,.5,.6], 'barked': [.7,.8,.9]}\n",
    "    make_embeddings_array3d(token_seqs, index_word_, embs_, 4, 3)\n",
    "    '''\n",
    "    make_embedding_matrix = partial(\n",
    "        token_idxs_to_embedding_matrix, \n",
    "        index_word=index_word, embs=embs, vec_size=vec_size)\n",
    "    \n",
    "    out = np.zeros(shape=(len(token_seqs), maxlen, vec_size))\n",
    "    \n",
    "    for idx in range(len(token_seqs)):\n",
    "        out[idx] = make_embedding_matrix(token_seqs[idx])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind args that won't vary to the make_embeddings_array3d() func \n",
    "seqs_to_embarray = partial(\n",
    "    make_embeddings_array3d, \n",
    "    index_word=tokenizer.index_word, embs=vocab_embs, \n",
    "    maxlen=maxlen, vec_size=vec_size)\n",
    "\n",
    "# TOO SLOW!!! NOT USABLE AT EVEN THIS SCALE </3\n",
    "embmats_train = seqs_to_embarray(x_train)\n",
    "embmats_test = seqs_to_embarray(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
